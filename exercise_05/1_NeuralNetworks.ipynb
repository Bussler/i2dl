{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and CIFAR10 Classification\n",
    "\n",
    "In the previous assignment you've taken a closer look to the binary classification scenario as well as the common steps that are shared between almost all deep learning projects, the so called \"solver\", using a simple logistic regression setup.\n",
    "\n",
    "In this exercise, we will first take a closer look on the model side. In particular, you will implement self-contained building blocks that allow us to build complex models with ease. Afterwards, we will tackle the full pipeline once again, using new knowledge from our lectures to solve a more tricky task, i.e., general/non-binary classification. In particular, we will be using the CIFAR10 dataset that you developed in exercise 3, though we have to make some changes on the optimization and loss side, in comparison to your toy example in exercise 4. \n",
    "\n",
    "The notebook is quite long but there are not too many implementation tasks. More often we implemented certain aspects for you to showcase some nice features, but you should also look at our implementations to see the specific details. For a first pass however, you can choose to ditch those and focus on the blue boxes. \n",
    "\n",
    "Let's get started!\n",
    "\n",
    "<img src=\"./images/2-layer-nn.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[0,1],[0,1]])\n",
    "b = np.array([1])\n",
    "c = np.array([b,b])\n",
    "\n",
    "a.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from exercise_code.data import (\n",
    "    DataLoader,\n",
    "    ImageFolderDataset,\n",
    "    RescaleTransform,\n",
    "    NormalizeTransform,\n",
    "    FlattenTransform,\n",
    "    ComposeTransform,\n",
    ")\n",
    "from exercise_code.networks import (\n",
    "    ClassificationNet,\n",
    "    CrossEntropyFromLogits\n",
    ")\n",
    "from exercise_code.tests.layer_tests import *\n",
    "from exercise_code.tests.sgdm_tests import *\n",
    "\n",
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.optimizer import (\n",
    "    SGD,\n",
    "    SGDMomentum,\n",
    "    Adam\n",
    ")\n",
    "from exercise_code.networks.compute_network_size import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Network Models\n",
    "\n",
    "In `Exercise 4` you have already defined and implemented your first model. In order to understand the required building parts, it is important that re-visit this structure as we intent to develop our model definition from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please re-read your implementation (in particular the necessary functions and variables) of our classifier class in Exercise 4 in <code>../exercise_04/exercise_code/networks/classifier.py</code>. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we defined three important components:\n",
    "- `__init__` and `initialize_weights`: that setup our classifier class, in particular we set up the weight matrix values here which we used for our logistic regression network\n",
    "- `forward`: that takes an input, **caches that input** and uses that input\n",
    "- `backward`: that computes gradient updates for our trained weight matrix\n",
    "\n",
    "\n",
    "## 1.1 Modularization\n",
    "\n",
    "The model you have implemented in `Exercise 4` was quite simple in nature, but if you would have to define multiple linear layer setups etc. it would get tricky quite quickly. Luckily, the lecture already provides us with a pleasant surprise that enables us to modularize the whole concepts of neural networks: the chain rule!\n",
    "\n",
    "<img src=\"./images/chainrule.png\">\n",
    "\n",
    "Same as the displayed image above, this fact is huge! It is the cornerstone of modern deep neural network building. Since, if we want to chain multiple linear layers (together with some auxiliary layers such as non-linearities) together, we have to compute their respective derivatives in order to update their weight matrices. However, thanks to the chain rule, we can create small building blocks that\n",
    "- in the `forward` pass do all required computations as well as save all values that are required to compute gradients and\n",
    "- in the `backward` function they will use the incoming gradients from later building blocks, to compute their respective gradients using their cached values.\n",
    "\n",
    "And therefore, we can just simply chain an arbitrary amount of such blocks, so called `layers`, together to create any structure we so desire (and finally which our hardware supports). Here is a more detailed code overview what we have to implement for those two passes:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "\n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "\n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "\n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "\n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "To check the correctness of your implementations below, we will again use numeric gradient checking\n",
    "\n",
    "$$ \\frac {df(x)}{dx} = \\frac{f(x+h) - f(x-h)}{2h} $$\n",
    "\n",
    "to compute values for layer outputs for your backward passees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Layer Example: Non-Linearities\n",
    "\n",
    "One of the simplest layers are non-linearities. They don't require any initial setup and we can easily write down their forward and backward passes.\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "Sigmoid is one of the oldest used non-linearities. You already implemented it in the last exercise together with other layers. For reference, here is the mathematical formula:\n",
    "$$Sigmoid(x) = \\frac{1}{1 + exp(-x)}$$\n",
    "and its activation graph\n",
    "\n",
    "<img src=https://pytorch.org/docs/stable/_images/Sigmoid.png alt=\"Figure4\" width=\"400\"/>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Open the file <code>exercise_code/networks/layer.py</code>. Implement the <code>forward</code> and the <code>backward</code> method in the <code>Sigmoid</code> class, and test your implementation by running the following cell.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SigmoidForwardTest passed.\n",
      "SigmoidBackwardTest passed.\n",
      "Congratulations! You have passed all the unit tests!!! Tests passed: 2/2\n",
      "Score: 100/100\n",
      "You secured a score of: 100\n"
     ]
    }
   ],
   "source": [
    "# Test your sigmoid implementation\n",
    "print(SigmoidTestWrapper()())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu\n",
    "\n",
    "Rectified Linear Units are the currently most used non-linearities in deep learning. They are not without flaws though they solve some issues introduced by non-linearities such as sigmoid above. Here is the formula\n",
    "$$ReLU(x) = max(0, x)$$\n",
    "and its clean graph\n",
    "<img src=https://pytorch.org/docs/stable/_images/ReLU.png alt=\"Figure2\" width=\"400\"/>\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Open the file <code>exercise_code/networks/layer.py</code>. Implement the <code>forward</code> and the <code>backward</code> method in the <code>Relu</code> class, and test your implementation by running the following cell.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReluForwardTest passed.\n",
      "ReluBackwardTest passed.\n",
      "Congratulations! You have passed all the unit tests!!! Tests passed: 2/2\n",
      "Score: 100/100\n",
      "You secured a score of: 100\n"
     ]
    }
   ],
   "source": [
    "# Test your ReLu implementation\n",
    "print(ReluTestWrapper()())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Affine Layers\n",
    "\n",
    "Our two layers above have no trainable weights and would make a poor network. As discussed in the lecture, Neural Networks use so called affine or linear layers that each has a learned weight matrix which we optimize in our training process.\n",
    "\n",
    "We deviate from our narrative here a little bit and focus our implementation efforts on the forward and backward passes of linear layers. Later on, you will encounter them in Pytorch again. There, they fully modularize those which makes network creation much simpler for the consumer/non-implementer ;).\n",
    "\n",
    "An affine layer computes a function of\n",
    "\n",
    "$$\\mathbf{z} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$$\n",
    "\n",
    "where $W$ is our learned weight matrix.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Open the file <code>exercise_code/networks/layer.py</code>. Implement the <code>affine_forward</code> and the <code>affine_backward</code> function and test your implementation by running the following cell.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AffineForwardTest passed.\n",
      "AffineBackwardTestDx passed.\n",
      "AffineBackwardTestDw passed.\n",
      "AffineBackwardTestDb passed.\n",
      "Congratulations! You have passed all the unit tests!!! Tests passed: 4/4\n",
      "Score: 100/100\n",
      "You secured a score of: 100\n"
     ]
    }
   ],
   "source": [
    "# Test your affine layer implementations\n",
    "print(AffineTestWrapper()())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 N-layer Classification Network\n",
    "\n",
    "Now that you have all necessary building blocks, you can build up your first neural network model. However, as stated above, we have omitted some details in the initialization of said network. We advise you to take a look at our network implementation to get a feel of how everything is computed.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please check the implementation in <code>exercise_code/networks/classification_net.py</code>.\n",
    " </p>\n",
    "</div>\n",
    "\n",
    "We can then simply call the network as shown below in the case of a rather small two layers network using two linear layers of size $128$ as well as Relu non-linearities in-between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# Define a dummy input\n",
    "test_input = np.random.randn(1, 10)   # (batch_size, input_size)\n",
    "\n",
    "# Define a test model\n",
    "test_model = ClassificationNet(input_size=10, \n",
    "                               hidden_size=128,\n",
    "                               activation=Relu(), \n",
    "                               num_layer=2, \n",
    "                               num_classes=3)\n",
    "\n",
    "# Compute output\n",
    "model_output = test_model.forward(test_input)\n",
    "print('Model output shape:', model_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CIFAR10 Dataset\n",
    "\n",
    "Great! Now that we modularized our layers and are able to build any networks based of linear layers as well as non-linearities, let's start checking out our data again!\n",
    "\n",
    "Let us first do some setup to be back at our state of after completing exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maarten\\Documents\\GitHub\\i2dl\\datasets\\cifar10\n"
     ]
    }
   ],
   "source": [
    "# Define output path similar to exercise 3\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "cifar_root = os.path.join(i2dl_exercises_path, \"datasets\", \"cifar10\")\n",
    "\n",
    "# Dictionary so that we can convert label indices to actual label names\n",
    "classes = [\n",
    "    'plane', 'car', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck',\n",
    "]\n",
    "\n",
    "# Simply call dataset class\n",
    "dataset = ImageFolderDataset(\n",
    "        root=cifar_root\n",
    "    )\n",
    "\n",
    "print(cifar_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up a dataset iterate over it and visualize images as well as labels easily just like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0\n",
      "image shape: (32, 32, 3)\n",
      "label: bird\n",
      "Sample 1\n",
      "image shape: (32, 32, 3)\n",
      "label: cat\n",
      "Sample 2\n",
      "image shape: (32, 32, 3)\n",
      "label: truck\n",
      "\n",
      "Sample images\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_images = 3\n",
    "\n",
    "for i in range(num_images):\n",
    "    item = dataset[i]\n",
    "    image = item['image']\n",
    "    label = item['label']\n",
    "    \n",
    "    # Print shape and label\n",
    "    print('Sample {}\\nimage shape: {}\\nlabel: {}'.format(\n",
    "        i, image.shape, classes[label]))\n",
    "    \n",
    "    # Visualize image\n",
    "    plt.subplot(1, num_images, 1 + i)\n",
    "    plt.imshow(image.astype('uint8'))\n",
    "\n",
    "print('\\nSample images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. We are basically done at this point bar transforms. In exercise 3 we made sure to normalize our images so we should do it here as well.\n",
    "\n",
    "However, we also have to consider our network to accommodate the dataset output to our network input. In our case, we require a one dimensional input vector. The simplest way to realize this is to flatten the vector which we do so by an additional transform and compose all of them together such as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Cifar10 mean and standard deviation computed in Exercise 3.\n",
    "cifar_mean = np.array([0.49191375, 0.48235852, 0.44673872])\n",
    "cifar_std  = np.array([0.24706447, 0.24346213, 0.26147554])\n",
    "\n",
    "# Define all the transforms we will apply on the images when \n",
    "# retrieving them.\n",
    "rescale_transform = RescaleTransform()\n",
    "normalize_transform = NormalizeTransform(\n",
    "    mean=cifar_mean,\n",
    "    std=cifar_std\n",
    ")\n",
    "\n",
    "# Add the new flatten transform\n",
    "flatten_transform = FlattenTransform()\n",
    "\n",
    "# And string them together\n",
    "compose_transform = ComposeTransform([\n",
    "    rescale_transform, \n",
    "    normalize_transform,\n",
    "    flatten_transform\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our outputs are sufficiently resized and we can apply our transforms on initializing our dataset. \n",
    "\n",
    "Before testing any deep learning pipeline, you should overfit on a small sample first which will be our goal for this notebook: we only take $1\\%$ (or 500 images) of our training data to overfit on later and set up our dataset and dataloader accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 500\n",
      "Dataloader size: 62\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset and dataloader\n",
    "batch_size = 8\n",
    "\n",
    "dataset = ImageFolderDataset(\n",
    "    mode='train',\n",
    "    root=cifar_root,\n",
    "    transform=compose_transform,\n",
    "    split={'train': 0.01, 'val': 0.2, 'test': 0.79}\n",
    ")\n",
    "    \n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print('Dataset size:', len(dataset))\n",
    "print('Dataloader size:', len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick check if the image input is sufficient. An image should be a single array of size $3*32*32$ and we should have `batch_size` of them per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 image shape: (8, 3072)\n",
      "Batch 1 image shape: (8, 3072)\n",
      "Batch 2 image shape: (8, 3072)\n"
     ]
    }
   ],
   "source": [
    "num_batches = 3\n",
    "\n",
    "for i, item in enumerate(dataloader):\n",
    "    image_batch = item['image']\n",
    "    print('Batch {} image shape: {}'.format(i, image_batch.shape))\n",
    "    \n",
    "    if i >= num_batches-1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we have to define a network that can work with those inputs. For simplicity, we are starting with a small two-layer neural network with two hidden layers of size 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 3072\n"
     ]
    }
   ],
   "source": [
    "# First get dataset entry for our network input size\n",
    "sample_image = dataset[0]['image']\n",
    "\n",
    "input_size = sample_image.shape[0]\n",
    "print('Input size:', input_size)\n",
    "\n",
    "# Define our model\n",
    "model = ClassificationNet(input_size=input_size, \n",
    "                          hidden_size=128,\n",
    "                          activation=Relu(), \n",
    "                          num_layer=2, \n",
    "                          num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to above we can use our dataloader to iterate over the dataloader and now apply our network on each batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 output shape (8, 10)\n",
      "Batch 1 output shape (8, 10)\n",
      "Batch 2 output shape (8, 10)\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the network using our dataloader\n",
    "for i, item in enumerate(dataloader):\n",
    "    images = item['image']\n",
    "    \n",
    "    output = model.forward(images)\n",
    "    \n",
    "    print('Batch {} output shape {}'.format(i, output.shape))\n",
    "    if i >= num_batches-1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's move on.\n",
    "\n",
    "# 3. Cross-Entropy/Softmax Loss from Logits\n",
    "\n",
    "There is still something left to fix before we can start though. In exercise 4 you tackled a binary problem. However, CIFAR10 consists of 10 classes. Therefore, we can't just simply use a binary objective function. Luckily, we can generalize our Binary Cross-Entropy Loss from exercise 4.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Softmax Loss</h3>\n",
    "    <p>This usually confuses people: in literature the general <b>Cross-Entropy</b> Loss is often simply called <b>Softmax Loss</b>, due to the Softmax activation function.\n",
    "         </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "\n",
    "$$ CE(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^{C} \\Big[ -y_{ik} \\log(\\hat{y}_{ik}) \\Big] $$\n",
    "\n",
    "where:\n",
    "- $ N $ is again the number of samples\n",
    "- $ C $ is the number of classes\n",
    "- $ \\hat{y}_{ik} $ is the probability that the model assigns for the $k$'th class when the $i$'th sample is the input. \n",
    "- $y_{ik} = 1 $ iff the true label of the $i$th sample is $k$ and 0 otherwise. This is called a [one-hot encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/).\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Formula</h3>\n",
    "    <p>Check for yourself that when the number of classes $ C $ is 2, then binary cross-entropy is actually equivalent to cross-entropy.</p>\n",
    "</div>\n",
    "\n",
    "## From Logits\n",
    "\n",
    "If we are not applying an activation function on the last layer of our network, its outputs for each sample will not be a valid probability distribution over the classes. We call these raw outputs of the network '[logits](https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean/31045)' and we will apply a [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation in order to obtain a valid \"probability distribution\".\n",
    "\n",
    "## Implementation\n",
    "We implemented the general formulation for you. There are some details you should check out:\n",
    "\n",
    "### Numerical Stability\n",
    "First check out the computation of the softmax itself. Recall that \n",
    "$$softmax(x)=\\sigma(x)=\\frac{e^{x_i}}{\\sum_{i=1}^ne^{x_i}}$$\n",
    "for a vector $x=(x_i)_{(1\\leq i\\leq n)}\\in\\mathbb{R}^n$.\n",
    "However, the sum in the divisor is problematic when using floating point numbers as the resulting fraction can be too small to be accurately represented and yield numerical instabilities.\n",
    "\n",
    "One solution would be to instead consider\n",
    "$$\\sigma(x-\\max_{1\\leq i\\leq n}x_i)$$\n",
    "\n",
    "which ensures that our divisor is small enough to avoid numerical instabilities.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Proof</h3>\n",
    "    <p>Think about why this solves the numerical stability problem and prove that $\\sigma(x)=\\sigma(x+c)$ for any constant vector $c\\in\\mathbb{R}^n$.</p>\n",
    "</div>\n",
    "\n",
    "With that proof, we can simply switch out the softmax computation with the new vector above and avoid numerical instabilities.\n",
    "\n",
    "### Matrix Notation\n",
    "\n",
    "In order to implement the whole procedure efficiently, we use numpy to evaluate the whole batch computation. This is non-trivial, though we implement it for you in this notebook such that you can save time. We highly urge you, however, to think about the implementation for yourself first and then check out our implementation of the cross-entropy loss, including its backward pass.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Check the implemenation of the Softmax Loss under <code>exercise_code/networks/loss/CrossEntropyFromLogits</code> including the backward pass and think through it yourself to understand the matrix operations. Finally, compare it to the binary cross-entropy implementation in the same file or your previous implementation from last week.</p>\n",
    "</div>\n",
    "\n",
    "### Sanity Check\n",
    "\n",
    "Let's quickly check if our loss formulation works as intended. Let's compute the loss of a random vector from our network defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of single image sample: 2.3026428776111447\n"
     ]
    }
   ],
   "source": [
    "# Set up loss\n",
    "loss_func = CrossEntropyFromLogits()\n",
    "\n",
    "# Sample input from a single image\n",
    "sample_image = dataset[0]['image']\n",
    "sample_label = dataset[0]['label']\n",
    "single_image_batch = np.expand_dims(sample_image, 0)\n",
    "single_label_batch = np.expand_dims(sample_label, 0)\n",
    "\n",
    "# Feed forward using our network\n",
    "model_output = model.forward(single_image_batch)\n",
    "\n",
    "# Loss computation\n",
    "computed_loss, _ = loss_func(model_output, single_label_batch)\n",
    "print('Loss of single image sample:', computed_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Reason</h3>\n",
    "    <p>Why do we expect our loss to be close to -log(0.1)? Explain briefly.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimization\n",
    "\n",
    "We are nearly there yet. The final piece of our pipeline is to define our optimization method. In this week's lecture, you've seen a variety of possibilities at which we should take a look now.\n",
    "\n",
    "## 4.1 Gradient Descent vs Stochastic Gradient Descent\n",
    "\n",
    "For demonstration sake, let us compute the memory required to do a full gradient descent update on our small CIFAR10 dataset consisting of 500 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our model again\n",
    "model = ClassificationNet(input_size=input_size, \n",
    "                          hidden_size=128,\n",
    "                          activation=Relu(), \n",
    "                          num_layer=2, \n",
    "                          num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do so, we have to calculate the forward caches as well as the computed gradients for each update step. Before we go into details of actual update methods, let us compute those manually.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Optional: Compute Network Size</h3>\n",
    "    <p>Our network itself is quite small, as a quick check you can compute the size of the network defined above yourself by summing up all network weights. Compare them against the values computed below.</p>\n",
    "</div>\n",
    "\n",
    "A more challenging task is to calculate the memory required to compute one network forward pass. Lets just quickly do this for a small batch with our small model. In order to populate the forward caches, we have to make a forward pass using a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss\n",
    "loss_func = CrossEntropyFromLogits()\n",
    "\n",
    "# Get a random batch of our dataloader with batch_size 8\n",
    "sample_batch = iter(dataloader).__next__()\n",
    "sample_images = sample_batch['image']\n",
    "sample_labels = sample_batch['label']\n",
    "\n",
    "# Compute model output\n",
    "model_output = model.forward(sample_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are always saving our inputs for each of our layers (in order to be able to backpropagate using them later) we amass quite a lot of values for our forward pass using 8 images. We've implemented a way to calculate those in bytes and you should check out this implementation as well as the network background.\n",
    "\n",
    "Using said function, we can calculate the network size as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding layer caches for forward pass:\n",
      "affine1 3147072\n",
      "sigmoid1 768\n",
      "affine2 18832\n",
      "\n",
      "Adding layer gradients for backward pass:\n",
      "W1 24\n",
      "b1 24\n",
      "W2 24\n",
      "b2 24\n",
      "\n",
      "Total number of bytes used by network for batch: 3.02MB\n"
     ]
    }
   ],
   "source": [
    "num_bytes = compute_network_pass_size(model)\n",
    "\n",
    "print('\\nTotal number of bytes used by network for batch:', GetHumanReadable(num_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, 3MB is not too shabby but also not a crazy high number. Your memory should easily handle it. However, we still have to add the memory required for the backward pass. In order to do so, we have to populate the gradients. Above you can see that those are only 24 bytes in size and that is because they are empty. Let's change that by doing one backward pass manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding layer caches for forward pass:\n",
      "affine1 3147072\n",
      "sigmoid1 768\n",
      "affine2 18832\n",
      "\n",
      "Adding layer gradients for backward pass:\n",
      "W1 3145840\n",
      "b1 1120\n",
      "W2 10352\n",
      "b2 176\n",
      "\n",
      "Total number of bytes used by network for batch: 6.03MB\n"
     ]
    }
   ],
   "source": [
    "# 1. Compute loss\n",
    "_ = loss_func.forward(model_output, sample_labels)\n",
    "# 2. Compute loss gradients\n",
    "dout = loss_func.backward(model_output, sample_labels)\n",
    "# 3. Backpropagate gradients through model\n",
    "_ = model.backward(dout)\n",
    "\n",
    "# Now calculate bytes again\n",
    "num_bytes = compute_network_pass_size(model)\n",
    "\n",
    "print('\\nTotal number of bytes used by network for batch:', GetHumanReadable(num_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Optional: Check Code</h3>\n",
    "    <p>Check our implementation to compute the size of a network forward pass in bytes in <code>exercise_code/networks/compute_network_size.py</code>, which simply sums up the caches values as well as gradients. You should also think about how and why those caches/gradients are populated using the steps above.</p>\n",
    "</div>\n",
    "\n",
    "Nice! That is the amount of memory required to do a full training forward and backward pass using our small batch. \n",
    "\n",
    "However, if we wanted to compute the memory required to do a full gradient update for the CIFAR10 dataset using our small network, you'd need..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of bytes used by network for the whole dataset 224.68TB\n"
     ]
    }
   ],
   "source": [
    "# A current batch consists of 8 images. The whole dataset would require 50000/8 times the amount of memory\n",
    "num_bytes = num_bytes * 50000 / 8\n",
    "\n",
    "print('Total number of bytes used by network for the whole dataset', GetHumanReadable(num_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is the case why we usually have to use stochastic approaches to train neural networks :). For smaller tasks however, you can not only use regular gradient descent but also second order optimization approaches. For more info about those we refer to the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 SGD\n",
    "\n",
    "We start off with plain Stochastic Gradient Descent (SGD) which we implemented for you.\n",
    "\n",
    "Now that we have an optimizer, we can execute the full pipeline using our pre-written solver which is similar to the one you wrote in the previous exercise.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Good Practice</h3>\n",
    "    <p>Always, always, always when starting a new project or defining a new network: <b>overfit on a small set first and then generalize</b>. The 500 images we are using here are already too many sample for most cases. Start with a single sample, then 10 and finally a few hundred. Don't cheap out on this step! More often, your network will fail to generalize properly and you have to first know if it has enough capacity to overfit and that the full training pipeline is working!</p>\n",
    "    <p>In order to run these experiments, you don't necessarily need a validation set. Just a few training samples are enough to make those checks!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine model and loss function\n",
    "model = ClassificationNet(input_size=input_size, \n",
    "                          hidden_size=128,\n",
    "                          activation=Relu(), \n",
    "                          num_layer=2, \n",
    "                          num_classes=10)\n",
    "\n",
    "loss_func = CrossEntropyFromLogits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 20) train loss: 2.302604; val loss: 2.302604\n",
      "(Epoch 2 / 20) train loss: 2.302558; val loss: 2.302304\n",
      "(Epoch 3 / 20) train loss: 2.302248; val loss: 2.302003\n",
      "(Epoch 4 / 20) train loss: 2.301942; val loss: 2.301674\n",
      "(Epoch 5 / 20) train loss: 2.301602; val loss: 2.301285\n",
      "(Epoch 6 / 20) train loss: 2.301237; val loss: 2.300842\n",
      "(Epoch 7 / 20) train loss: 2.300703; val loss: 2.300326\n",
      "(Epoch 8 / 20) train loss: 2.300091; val loss: 2.299611\n",
      "(Epoch 9 / 20) train loss: 2.299297; val loss: 2.298600\n",
      "(Epoch 10 / 20) train loss: 2.298216; val loss: 2.297416\n",
      "(Epoch 11 / 20) train loss: 2.296769; val loss: 2.295809\n",
      "(Epoch 12 / 20) train loss: 2.294898; val loss: 2.293460\n",
      "(Epoch 13 / 20) train loss: 2.292433; val loss: 2.290695\n",
      "(Epoch 14 / 20) train loss: 2.288871; val loss: 2.286681\n",
      "(Epoch 15 / 20) train loss: 2.284614; val loss: 2.281139\n",
      "(Epoch 16 / 20) train loss: 2.278405; val loss: 2.274501\n",
      "(Epoch 17 / 20) train loss: 2.271579; val loss: 2.266485\n",
      "(Epoch 18 / 20) train loss: 2.262901; val loss: 2.256880\n",
      "(Epoch 19 / 20) train loss: 2.252260; val loss: 2.245401\n",
      "(Epoch 20 / 20) train loss: 2.242185; val loss: 2.234374\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "# We use our training dataloader for validation as well as testing\n",
    "solver = Solver(model, dataloader, dataloader, \n",
    "                learning_rate=learning_rate, loss_func=loss_func, optimizer=SGD)\n",
    "\n",
    "# This might take a while depending on your hardware. When in doubt: use google colab\n",
    "solver.train(epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 SGD + Momentum\n",
    "\n",
    "As you can see, the loss is going down smoothly which indicates that we are easily overfitting. Great. However, plain SGD is rarely used in practice (as it is usually too slow) which is why we will focus on implementing SGD+Momentum now, which is a straightforward extension to SGD.\n",
    "\n",
    "Recall that its update rule is defined by:\n",
    "\n",
    "$$ v^{k+1} = \\beta v^{k} - \\alpha \\nabla_{\\theta} L (\\theta^{k}),$$\n",
    "$$ \\theta^{k+1} = \\theta^{k} + v^{k+1}.$$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Check Code and Implement </h3>\n",
    "    <p>Familiarize yourself with the SGD implementation in <code>exercise_code/networks/optimizer.py</code> as well as our general optimization class structure.</p>\n",
    "    <p> Then, implement the <code>SGDMomentum._update</code> function which is very similar to the update rule of SGD above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDM_Weight_Test passed.\n",
      "SGDM_Velocity_Test passed.\n",
      "Congratulations you have passed all the unit tests!!! Tests passed: 2/2\n",
      "Score: 100/100\n",
      "You secured a score of :100\n"
     ]
    }
   ],
   "source": [
    "#Test your SGD momentum implementations\n",
    "print(SGDMTestWrapper()())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Adam\n",
    "\n",
    "There are a variety of updates that people have suggested for SGD. For a more detailed overview of optimizers and their development, we refer to [this educational blog post](https://ruder.io/optimizing-gradient-descent/).\n",
    "\n",
    "Besides SGD+Momentum (which is still used quite often and you should never discard it), the currently most used optimizer however is Adam. In comparison to SGD, it uses a first and second order momentum.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>We have implemented Adam's update rule for you and you can check out the implementation in <code>exercise_code/networks/optimizer.py</code>.</p>\n",
    "</div>\n",
    "\n",
    "## 4.5 Optimizer Comparison\n",
    "\n",
    "Finally, let's compare our different optimization methods against each other on our small testing dataloader with shared parameters. That can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SGD\n",
      "(Epoch 1 / 20) train loss: 2.302611; val loss: 2.302613\n",
      "(Epoch 2 / 20) train loss: 2.302609; val loss: 2.302583\n",
      "(Epoch 3 / 20) train loss: 2.302581; val loss: 2.302556\n",
      "(Epoch 4 / 20) train loss: 2.302553; val loss: 2.302527\n",
      "(Epoch 5 / 20) train loss: 2.302523; val loss: 2.302501\n",
      "(Epoch 6 / 20) train loss: 2.302500; val loss: 2.302479\n",
      "(Epoch 7 / 20) train loss: 2.302463; val loss: 2.302451\n",
      "(Epoch 8 / 20) train loss: 2.302437; val loss: 2.302419\n",
      "(Epoch 9 / 20) train loss: 2.302410; val loss: 2.302392\n",
      "(Epoch 10 / 20) train loss: 2.302391; val loss: 2.302360\n",
      "(Epoch 11 / 20) train loss: 2.302358; val loss: 2.302342\n",
      "(Epoch 12 / 20) train loss: 2.302320; val loss: 2.302302\n",
      "(Epoch 13 / 20) train loss: 2.302303; val loss: 2.302279\n",
      "(Epoch 14 / 20) train loss: 2.302276; val loss: 2.302256\n",
      "(Epoch 15 / 20) train loss: 2.302259; val loss: 2.302226\n",
      "(Epoch 16 / 20) train loss: 2.302203; val loss: 2.302191\n",
      "(Epoch 17 / 20) train loss: 2.302190; val loss: 2.302171\n",
      "(Epoch 18 / 20) train loss: 2.302174; val loss: 2.302140\n",
      "(Epoch 19 / 20) train loss: 2.302135; val loss: 2.302114\n",
      "(Epoch 20 / 20) train loss: 2.302107; val loss: 2.302085\n",
      "\n",
      "Starting SGD+Momentum\n",
      "(Epoch 1 / 20) train loss: 2.302620; val loss: 2.302620\n",
      "(Epoch 2 / 20) train loss: 2.302578; val loss: 2.302377\n",
      "(Epoch 3 / 20) train loss: 2.302311; val loss: 2.302113\n",
      "(Epoch 4 / 20) train loss: 2.302025; val loss: 2.301784\n",
      "(Epoch 5 / 20) train loss: 2.301715; val loss: 2.301473\n",
      "(Epoch 6 / 20) train loss: 2.301336; val loss: 2.301062\n",
      "(Epoch 7 / 20) train loss: 2.300989; val loss: 2.300626\n",
      "(Epoch 8 / 20) train loss: 2.300477; val loss: 2.300080\n",
      "(Epoch 9 / 20) train loss: 2.299882; val loss: 2.299436\n",
      "(Epoch 10 / 20) train loss: 2.299058; val loss: 2.298469\n",
      "(Epoch 11 / 20) train loss: 2.298042; val loss: 2.297308\n",
      "(Epoch 12 / 20) train loss: 2.296667; val loss: 2.295681\n",
      "(Epoch 13 / 20) train loss: 2.294779; val loss: 2.293529\n",
      "(Epoch 14 / 20) train loss: 2.292179; val loss: 2.290446\n",
      "(Epoch 15 / 20) train loss: 2.288832; val loss: 2.286855\n",
      "(Epoch 16 / 20) train loss: 2.284690; val loss: 2.281581\n",
      "(Epoch 17 / 20) train loss: 2.279640; val loss: 2.275808\n",
      "(Epoch 18 / 20) train loss: 2.272524; val loss: 2.267985\n",
      "(Epoch 19 / 20) train loss: 2.264222; val loss: 2.260043\n",
      "(Epoch 20 / 20) train loss: 2.254704; val loss: 2.248670\n",
      "\n",
      "Starting Adam\n",
      "(Epoch 1 / 20) train loss: 2.302614; val loss: 2.302616\n",
      "(Epoch 2 / 20) train loss: 2.182266; val loss: 1.837090\n",
      "(Epoch 3 / 20) train loss: 1.880652; val loss: 1.463630\n",
      "(Epoch 4 / 20) train loss: 1.593871; val loss: 1.242335\n",
      "(Epoch 5 / 20) train loss: 1.292263; val loss: 0.984482\n",
      "(Epoch 6 / 20) train loss: 1.025655; val loss: 0.667783\n",
      "(Epoch 7 / 20) train loss: 0.805118; val loss: 0.554261\n",
      "(Epoch 8 / 20) train loss: 0.541449; val loss: 0.395882\n",
      "(Epoch 9 / 20) train loss: 0.430523; val loss: 0.210506\n",
      "(Epoch 10 / 20) train loss: 0.229221; val loss: 0.137662\n",
      "(Epoch 11 / 20) train loss: 0.136144; val loss: 0.080886\n",
      "(Epoch 12 / 20) train loss: 0.120964; val loss: 0.107077\n",
      "(Epoch 13 / 20) train loss: 0.153606; val loss: 0.079397\n",
      "(Epoch 14 / 20) train loss: 0.090301; val loss: 0.059156\n",
      "(Epoch 15 / 20) train loss: 0.054697; val loss: 0.027410\n",
      "(Epoch 16 / 20) train loss: 0.023171; val loss: 0.014869\n",
      "(Epoch 17 / 20) train loss: 0.017548; val loss: 0.011955\n",
      "(Epoch 18 / 20) train loss: 0.013255; val loss: 0.018898\n",
      "(Epoch 19 / 20) train loss: 0.022447; val loss: 0.010396\n",
      "(Epoch 20 / 20) train loss: 0.010111; val loss: 0.008191\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 20\n",
    "loss_func = CrossEntropyFromLogits()\n",
    "\n",
    "# Compute loss histories for all optimizers\n",
    "loss_histories = {}\n",
    "\n",
    "for name, optimizer in zip(['SGD', 'SGD+Momentum', 'Adam'], [SGD, SGDMomentum, Adam]):\n",
    "    print('Starting {}'.format(name))\n",
    "    # Reset model\n",
    "    model = ClassificationNet(input_size=input_size, \n",
    "                              hidden_size=128,\n",
    "                              activation=Relu(), \n",
    "                              num_layer=2, \n",
    "                              num_classes=10)\n",
    "    # Set up solver\n",
    "    solver = Solver(model, dataloader, dataloader, \n",
    "                    learning_rate=learning_rate, loss_func=loss_func,\n",
    "                    optimizer=optimizer)\n",
    "    solver.train(epochs=num_epochs)\n",
    "    # Save train history to plot later\n",
    "    loss_histories[name] = solver.train_loss_history\n",
    "    print()\n",
    "\n",
    "# Plot them in a shared plot\n",
    "for name in loss_histories:\n",
    "    plt.plot(loss_histories[name], '-', label=name)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew... that was a lot of work, but with this plot exercise 5 is done :).\n",
    "\n",
    "# 6. Outlook\n",
    "\n",
    "Should you always use Adam, as this single experiment using a fixed set of parameters suggests? How would you perform on unseen data using this network? Which network is the best?\n",
    "\n",
    "All of these questions have a shared condition: **hyperparameters**! Basically everything we touched in this notebook is a hyperparameter for a deep learning task:\n",
    "- the network architecture,\n",
    "- data transformations,\n",
    "- optimizer,\n",
    "- and much more.\n",
    "\n",
    "But how do you choose good hyperparameters? They will not magically appear and since we are computer scientists we are allergic to excessive manual testing. Therefore, we will show ways to come up with them in an automated manner in our next exercise, so stay tuned :).\n",
    "\n",
    "# 7. Submission Instructions\n",
    "Hooooooray, you trained your model! The model will be saved as a pickle file to `models/NN.p`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.tests import save_pickle\n",
    "from exercise_code.networks.layer import *\n",
    "from exercise_code.networks.optimizer import SGDMomentum\n",
    "\n",
    "save_pickle(\n",
    "    data_dict={\n",
    "        \"SGD_Momentum_update\": SGDMomentum._update,\n",
    "        \"AffineForward\": affine_forward,\n",
    "        \"AffineBackward\": affine_backward,\n",
    "        \"Sigmoid\": Sigmoid,\n",
    "        \"Relu\": Relu,\n",
    "    },\n",
    "    file_name=\"NN.p\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that you have completed the necessary parts in the notebook, you can go on and submit your files.\n",
    "\n",
    "1. Go on [our submission page](https://i2dl.vc.in.tum.de/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum-online and check your mails there. You will get an id which we need in the next step.\n",
    "2. Log into [our submission page](https://i2dl.vc.in.tum.de/) with your account details and upload the zip file.\n",
    "3. Your submission will be evaluated by our system and you will get feedback about the performance of it. You will get an email with your score as well as a message if you have surpassed the threshold.\n",
    "4. Within the working period, you can submit as many solutions as you want to get the best possible score.\n",
    "\n",
    "Simply run the following cell to create a zipped file for your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.submit import submit_exercise\n",
    "\n",
    "\n",
    "submit_exercise('exercise05')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Goals\n",
    "\n",
    "- Goals: Implement all implementation tasks.\n",
    "- Test cases: \n",
    "    1. Does `forward()` of `Sigmoid` layer return the correct value during forward pass?\n",
    "    2. Does `backward()` of `Sigmoid` layer return the correct value during backward pass?\n",
    "    3. Does `forward()` of `Relu` layer return the correct value during forward pass?\n",
    "    4. Does `backward()` of `Relu` layer return the correct value during backward pass?\n",
    "    5. Does `affine_forward()` return the correct value of `out`?\n",
    "    6. Does `affine_backward()` return the correct value of `dx`?\n",
    "    7. Does `affine_backward()` return the correct value of `dw`?\n",
    "    8. Does `affine_backward()` return the correct value of `db`?\n",
    "    9. Does `update()` of SGDMomentum return the correct weight values after update?\n",
    "    10. Does `update()` of SGDMomentum return the correct velocity values after update?\n",
    "\n",
    "- Reachable points [0, 100]: 0 if not implemented, 100 if all tests passed, 10 per passed test\n",
    "- Threshold to clear exercise: 100\n",
    "- Submission start: __Novewmber 18, 2021, 13.00__\n",
    "- Submission deadline : __Novewmber 24, 2021, 15.59__ \n",
    "- You can make multiple submission until the deadline. Your __best submission__ will be considered for bonus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
